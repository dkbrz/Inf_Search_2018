{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все работает, но не все в итоге полноценно запущено, в ДЗ 4 точно так же все и оно работает."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument, TaggedLineDocument\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "punkt = string.punctuation+'»«–…'\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "from judicial_splitter import splitter\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import zip_longest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapdict import heapdict\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text, stop=False):\n",
    "    global word_tokenize, stop_words, punkt\n",
    "    text = re.sub(r\"([a-zа-я0-9])(.)([A-ZА-Я0-9])\", r\"\\1\\2 \\3\", text)\n",
    "    text = word_tokenize(text)\n",
    "    new_text= []\n",
    "    for word in text:\n",
    "        word = word.strip(punkt)\n",
    "        if word:\n",
    "            if word in punkt: continue\n",
    "            elif word.isdigit(): continue\n",
    "            elif stop and word in stop_words: continue\n",
    "            else: new_text.append(morph.parse(word)[0].normal_form)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write(path):\n",
    "    global qa_corpus\n",
    "    with open('d2v_answers_3.txt','w', encoding = 'utf-8') as answers:\n",
    "        with open ('d2v_indexes.txt', 'w', encoding = 'utf-8') as indexes:\n",
    "            for root, dirs, files in list(os.walk(path)):\n",
    "                for file in tqdm(files):\n",
    "                    with open (os.path.join(root, file), 'r', encoding='utf-8') as f:\n",
    "                        article = f.read()\n",
    "                    for chunk in splitter(article, 3):\n",
    "                        answer = ' '.join(preprocessing(chunk))\n",
    "                        if len(answer) > 5:\n",
    "                            answers.write(answer+'\\n')\n",
    "                            indexes.write(str(file.split('.')[-2])+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Записать всё в предобработанном варианте, чтобы быстрее проверять."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96c89cdd6bad4055aecb9ff802164fb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=173359), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "write('/home/dkbrz/data/article/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D2V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итерирование по файлу вместо хранения при обучении."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs():\n",
    "    for key, value in enumerate(zip_longest(open('d2v_indexes.txt','r'), open('d2v_answers_3.txt','r'))):\n",
    "        yield TaggedDocument(words=value[1].strip().split(), tags=[int(value[0].strip())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object docs at 0x7fd5800a6200>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "    def __iter__(self):\n",
    "        for key, value in enumerate(zip_longest(open('d2v_indexes.txt','r'), open('d2v_answers_3.txt','r'))):\n",
    "            yield TaggedDocument(words=value[1].strip().split(), tags=[int(value[0].strip())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 38.4 s, sys: 526 ms, total: 39 s\n",
      "Wall time: 39 s\n",
      "75401\n",
      "CPU times: user 14h 32min 30s, sys: 1h 6min 36s, total: 15h 39min 6s\n",
      "Wall time: 6h 36min 13s\n"
     ]
    }
   ],
   "source": [
    "d2v_model = Doc2Vec(vector_size=100, min_count=5, alpha=0.025, seed = 23,\n",
    "                min_alpha=0.025, epochs=200, workers=8, dm=1)\n",
    "\n",
    "%time d2v_model.build_vocab(LabeledLineSentence(''))\n",
    "print (len(d2v_model.wv.vocab))\n",
    "%time d2v_model.train(LabeledLineSentence(''), total_examples=d2v_model.corpus_count, epochs=d2v_model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_model.save('d2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_model = Doc2Vec.load('d2v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d2v_search(query, n=5):\n",
    "    global d2v_model\n",
    "    d2v_model.random.seed(23)\n",
    "    d2v_vector = d2v_model.infer_vector(query.strip().split(), epochs=1000)\n",
    "    result = {i[0]: i[1] for i in d2v_model.docvecs.most_similar(positive = [d2v_vector], topn=n)}\n",
    "    return dict(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def save_d2v_base():\n",
    "    global d2v_model\n",
    "    with open('d2v_vectors','w', encoding = 'utf-8') as vectors:\n",
    "        for line in tqdm(open('d2v_answers_3.txt','r')):\n",
    "            d2v_model.random.seed(23)\n",
    "            d2v = d2v_model.infer_vector(line.strip().split())\n",
    "            d2v = d2v.tolist()\n",
    "            vectors.write(json.dumps(d2v)+'\\n')\n",
    "%time save_d2v_base()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d2v_search(query, n=10):\n",
    "    global d2v_model\n",
    "    result = heapdict()\n",
    "    d2v_model.random.seed(23)\n",
    "    q_d2v = d2v_model.infer_vector(query.strip().split())\n",
    "    for key, value in enumerate(zip_longest(open('d2v_indexes.txt','r'), open('d2v_vectors','r'))):\n",
    "        d2v = json.loads(value[1])\n",
    "        x = cosine_similarity([q_d2v], [d2v])[0][0]\n",
    "        #print (x)\n",
    "        ordinal = int(value[0])\n",
    "        if ordinal in result:\n",
    "            if result[ordinal] < x:\n",
    "                result[ordinal] = x\n",
    "        else:\n",
    "            if len(result) == n:\n",
    "                z = result.peekitem()\n",
    "                if x > z[1]:\n",
    "                    result.popitem()\n",
    "                    result[ordinal] = x\n",
    "            else:\n",
    "                result[ordinal] = x\n",
    "    return dict(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W2V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w2v_vectors(text, k=300, prep=True):\n",
    "    \"\"\"Получает вектор документа\"\"\"\n",
    "    \n",
    "    global w2v_model, stop_words, word_tokenize\n",
    "    \n",
    "    if prep:\n",
    "        arr_text = preprocessing(text, stop=True)\n",
    "    else:\n",
    "        arr_text = text.split()\n",
    "    n = 0\n",
    "    vector = np.array([0]*300)\n",
    "    \n",
    "    for word in arr_text:\n",
    "        if word not in stop_words:\n",
    "            try:\n",
    "                vec = np.array(w2v_model.wv[word])\n",
    "                n += 1 \n",
    "                vector = vector + vec\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "    if n > 0: vector = vector / n\n",
    "    \n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec.load('/home/dkbrz/data/rusvectores/araneum_none_fasttextcbow_300_5_2018.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_search(query, n=10):\n",
    "    result = heapdict()\n",
    "    q_w2v = get_w2v_vectors(query, k=300, prep=False)\n",
    "    for key, value in enumerate(zip_longest(open('d2v_indexes.txt','r'), open('w2v_vectors','r'))):\n",
    "        w2v = json.loads(value[1])\n",
    "        x = cosine_similarity([q_w2v], [w2v])[0][0]\n",
    "        #print (x)\n",
    "        ordinal = int(value[0])\n",
    "        if ordinal in result:\n",
    "            if result[ordinal] < x:\n",
    "                result[ordinal] = x\n",
    "        else:\n",
    "            if len(result) == n:\n",
    "                z = result.peekitem()\n",
    "                if x > z[1]:\n",
    "                    result.popitem()\n",
    "                    result[ordinal] = x\n",
    "            else:\n",
    "                result[ordinal] = x\n",
    "    return dict(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05b666042c7a41f9a0033e8e71feb08b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "def save_w2v_base():\n",
    "    with open('w2v_vectors','w', encoding = 'utf-8') as vectors:\n",
    "        for line in tqdm(open('d2v_answers_3.txt','r')):\n",
    "            w2v = get_w2v_vectors(line.strip(), k=300, prep=False)\n",
    "            w2v = w2v.tolist()\n",
    "            vectors.write(json.dumps(w2v)+'\\n')\n",
    "%time save_w2v_base()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OKAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dd636062852407ca22f57551bed86b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1384), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def write():\n",
    "    global qa_corpus\n",
    "    with open('ok_answers.txt','w', encoding = 'utf-8') as answers:\n",
    "                for key, value in enumerate(tqdm(qa_corpus)):\n",
    "                        answer = ' '.join(preprocessing(value[1]))\n",
    "                        answers.write(answer+'\\n')\n",
    "write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_term_doc_matrix():\n",
    "    n = 1384\n",
    "    dictionary = {}\n",
    "    term_doc_matrix = []\n",
    "    for key, item in enumerate(open('ok_answers.txt','r')):\n",
    "        text = Counter(item.strip().split())\n",
    "        for word in text:\n",
    "            if word in dictionary:\n",
    "                term_doc_matrix[dictionary[word]][key] += text[word]\n",
    "            else:\n",
    "                dictionary[word] = len(dictionary)\n",
    "                term_doc_matrix.append(np.zeros(n))\n",
    "                term_doc_matrix[dictionary[word]][key] += text[word]\n",
    "    return dictionary, term_doc_matrix\n",
    "\n",
    "def inverted_index(dictionary, term_doc_matrix) -> dict:\n",
    "    \"\"\"\n",
    "    Create inverted index by input doc collection\n",
    "    :return: inverted index\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "    for word in dictionary:\n",
    "        result[word] = {key: int(value) for key, value in enumerate(term_doc_matrix[dictionary[word]]) if value > 0}\n",
    "    return result\n",
    "\n",
    "def score_BM25(qf, dl, avgdl, k1, b, N, n) -> float:\n",
    "    \"\"\"\n",
    "    Compute similarity score between search query and documents from collection\n",
    "    :return: score\n",
    "    \"\"\"\n",
    "    score = math.log(1 + (N-n+0.5)/(n+0.5)) * (k1+1)*qf/(qf+k1*(1-b+b*(dl/avgdl)))\n",
    "    return score\n",
    "\n",
    "def compute_sim(word, index, dictionary, term_doc_matrix, doc_length, avgdl, N) -> float:\n",
    "    \"\"\"\n",
    "    Compute similarity score between search query and documents from collection\n",
    "    :return: score\n",
    "    \"\"\"\n",
    "    if word in dictionary:\n",
    "        n = len(index[word])\n",
    "        result = {}\n",
    "        for doc in index[word]:\n",
    "            qf = term_doc_matrix[dictionary[word]][doc]/doc_length[doc]\n",
    "            #qf = term_doc_matrix[dictionary[word]][doc]\n",
    "            score = score_BM25(qf, doc_length[doc], avgdl, k1, b, N, n)\n",
    "            result[doc] = score\n",
    "        return result\n",
    "    else:\n",
    "        return {}\n",
    "\n",
    "def get_okapi(query, n = 30) -> float:\n",
    "    \"\"\"\n",
    "    Compute sim score between search query and all documents in collection\n",
    "    Collect as pair (doc_id, score)\n",
    "    :param query: input text\n",
    "    :return: list of lists with (doc_id, score)\n",
    "    \"\"\"\n",
    "    global index, dictionary, term_doc_matrix, doc_length, avgdl, N\n",
    "    query = query.strip().split()\n",
    "    result = defaultdict(int)\n",
    "    for word in query:\n",
    "        current = compute_sim(word, index, dictionary, term_doc_matrix, doc_length, avgdl, N)\n",
    "        for doc in current:\n",
    "            result[doc] += current[doc]\n",
    "    return {i[0]:i[1] for i in sorted(result.items(), key = lambda x: x[1], reverse = True)[:n]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary, term_doc_matrix = get_term_doc_matrix()\n",
    "index = inverted_index(dictionary, term_doc_matrix)\n",
    "\n",
    "k1 = 2.0\n",
    "b = 0.75\n",
    "\n",
    "doc_length = {}\n",
    "for key, value in enumerate(np.transpose(term_doc_matrix)):\n",
    "    doc_length[key] = sum(value)\n",
    "\n",
    "avgdl = sum(doc_length.values())/len(doc_length)\n",
    "N = len(doc_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_search(query, n=10, prep=True):\n",
    "    if prep: query = ' '.join(preprocessing(query))\n",
    "    w2v = w2v_search(query, n=250)\n",
    "    d2v = d2v_search(query, n=250)\n",
    "    okapi = get_okapi(query, n=250)\n",
    "    candidates = set(w2v) | set(d2v) | set(okapi)\n",
    "    result = heapdict()\n",
    "    #print (candidates)\n",
    "    for i in candidates:\n",
    "        if i in okapi: x = okapi[i]\n",
    "        else: x = 1\n",
    "        coef = 0\n",
    "        if i in w2v: coef += w2v[i]\n",
    "        if i in d2v: coef += d2v[i]\n",
    "        coef = coef*(1+math.log(1 +x))\n",
    "        if len(result) == n and coef > result.peekitem()[1]:\n",
    "            result.popitem()\n",
    "            result[i] = coef*(1+math.log(x))\n",
    "        elif len(result) < n: result[i] = coef*(1+math.log(x))\n",
    "    return dict(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_search(query, n=10, prep=True):\n",
    "    if prep: query = ' '.join(preprocessing(query))\n",
    "    w2v = w2v_search(query, n=250)\n",
    "    #d2v = d2v_search(query, n=250)\n",
    "    okapi = get_okapi(query, n=250)\n",
    "    #candidates = set(w2v) | set(d2v) | set(okapi)\n",
    "    candidates = set(w2v) | set(okapi)\n",
    "    result = heapdict()\n",
    "    #print (candidates)\n",
    "    for i in candidates:\n",
    "        if i in okapi: x = okapi[i]\n",
    "        else: x = 1\n",
    "        coef = 0\n",
    "        if i in w2v: coef += w2v[i]\n",
    "        #if i in d2v: coef += d2v[i]\n",
    "        coef = coef*(1+math.log(1 +x))\n",
    "        if len(result) == n and coef > result.peekitem()[1]:\n",
    "            result.popitem()\n",
    "            result[i] = coef*(1+math.log(x))\n",
    "        elif len(result) < n: result[i] = coef*(1+math.log(x))\n",
    "    return dict(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_search(query, n=10, prep=True):\n",
    "    if prep: query = ' '.join(preprocessing(query))\n",
    "    w2v = w2v_search(query, n=500)\n",
    "    #d2v = d2v_search(query, n=250)\n",
    "    okapi = get_okapi(query, n=500)\n",
    "    #candidates = set(w2v) | set(d2v) | set(okapi)\n",
    "    candidates = set(w2v) | set(okapi)\n",
    "    result = heapdict()\n",
    "    #print (candidates)\n",
    "    for i in candidates:\n",
    "        if i in okapi: x = okapi[i]\n",
    "        else: x = 1\n",
    "        coef = 0\n",
    "        if i in w2v: coef += w2v[i]\n",
    "        #if i in d2v: coef += d2v[i]\n",
    "        coef = coef*(1+math.log(1 +x))\n",
    "        if len(result) == n and coef > result.peekitem()[1]:\n",
    "            result.popitem()\n",
    "            result[i] = coef*(1+math.log(x))\n",
    "        elif len(result) < n: result[i] = coef*(1+math.log(x))\n",
    "    return dict(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_search(query, n=10, prep=True):\n",
    "    if prep: query = ' '.join(preprocessing(query))\n",
    "    w2v = w2v_search(query, n=500)\n",
    "    #d2v = d2v_search(query, n=250)\n",
    "    okapi = get_okapi(query, n=500)\n",
    "    #candidates = set(w2v) | set(d2v) | set(okapi)\n",
    "    candidates = set(w2v) | set(okapi)\n",
    "    result = heapdict()\n",
    "    #print (candidates)\n",
    "    for i in candidates:\n",
    "        if i in okapi: x = okapi[i]\n",
    "        else: x = 1\n",
    "        coef = 0\n",
    "        if i in w2v: coef += w2v[i]\n",
    "        #if i in d2v: coef += d2v[i]\n",
    "        coef = math.exp(coef)*x\n",
    "        if len(result) == n and coef > result.peekitem()[1]:\n",
    "            result.popitem()\n",
    "            result[i] = coef*(1+math.log(x))\n",
    "        elif len(result) < n: result[i] = coef*(1+math.log(x))\n",
    "    return dict(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best(result):\n",
    "    i = max(result, key=result.get)\n",
    "    with open('/home/dkbrz/data/article/{}.txt'.format(i), 'r', encoding='utf-8') as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nПЕРЕСМОТРЕ Судья Высшего Арбитражного Суда Российской Федерации Бондаренко С.П., рассмотрев заявление общества с ограниченной ответственностью \"Нева-Ресурс\" от 10.11.2011 б/н о пересмотре в порядке надзора решения Арбитражного суда города Санкт-Петербурга и Ленинградской области от 02.12.2010 по делу N А56-31154/2010 и постановления Федерального арбитражного суда Северо-Западного округа от 21.09.2011 по тому же делу, установил: заявление подано с соблюдением требований, предусмотренных статьями 292, 294 Арбитражного процессуального кодекса Российской Федерации. Руководствуясь статьей 295 Арбитражного процессуального кодекса Российской Федерации, судья Высшего Арбитражного Суда Российской Федерации определил: принять заявление общества с ограниченной ответственностью \"Нева-Ресурс\" от 10.11.2011 б/н о пересмотре в порядке  надзора решения Арбитражного суда города Санкт-Петербурга  и Ленинградской области от 02.12.2010 по делу N А56-31154/2010 и постановления Федерального арбитражного суда Северо-Западного округа от 21.09.2011 по тому же делу и возбудить надзорное производство. Судья С.П.БОНДАРЕНКО'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best({'4085':2, '345':50})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
